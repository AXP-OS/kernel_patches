From 496f87a02b2de244ed57ec848c02b2ac463f8db0 Mon Sep 17 00:00:00 2001
From: Daniel Micay <danielmicay@gmail.com>
Date: Wed, 3 May 2017 21:54:56 -0400
Subject: [PATCH] mm: add support for verifying page sanitization

Signed-off-by: Daniel Micay <danielmicay@gmail.com>
Signed-off-by: Thibaut Sautereau <thibaut.sautereau@ssi.gouv.fr>
Signed-off-by: Levente Polyak <levente@leventepolyak.net>
Signed-off-by: anupritaisno1 <www.anuprita804@gmail.com>
Signed-off-by: randomhydrosol <randomhydrosol@glassrom.org>
---
 include/linux/highmem.h    | 7 +++++++
 kernel/power/snapshot.c    | 5 +++--
 mm/page_alloc.c            | 6 ++++++
 security/Kconfig.hardening | 7 +++++++
 4 files changed, 23 insertions(+), 2 deletions(-)

diff --git a/include/linux/highmem.h b/include/linux/highmem.h
index b471a881f326..6de87c1a648c 100644
--- a/include/linux/highmem.h
+++ b/include/linux/highmem.h
@@ -208,6 +208,13 @@ static inline void clear_highpage(struct page *page)
 	kunmap_atomic(kaddr);
 }
 
+static inline void verify_zero_highpage(struct page *page)
+{
+	void *kaddr = kmap_atomic(page);
+	BUG_ON(memchr_inv(kaddr, 0, PAGE_SIZE));
+	kunmap_atomic(kaddr);
+}
+
 static inline void zero_user_segments(struct page *page,
 	unsigned start1, unsigned end1,
 	unsigned start2, unsigned end2)
diff --git a/kernel/power/snapshot.c b/kernel/power/snapshot.c
index d4dbe4a6d78f..953f266d2121 100644
--- a/kernel/power/snapshot.c
+++ b/kernel/power/snapshot.c
@@ -1143,10 +1143,12 @@ void free_basic_memory_bitmaps(void)
 
 void clear_free_pages(void)
 {
-#ifdef CONFIG_PAGE_POISONING_ZERO
 	struct memory_bitmap *bm = free_pages_map;
 	unsigned long pfn;
 
+	if (!IS_ENABLED(CONFIG_PAGE_POISONING_ZERO) && !want_init_on_free())
+		return;
+
 	if (WARN_ON(!(free_pages_map)))
 		return;
 
@@ -1160,7 +1162,6 @@ void clear_free_pages(void)
 	}
 	memory_bm_position_reset(bm);
 	pr_info("PM: free pages cleared after restore\n");
-#endif /* PAGE_POISONING_ZERO */
 }
 
 /**
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index b5cd0f6f4770..d5f2c01c0d14 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1856,6 +1856,12 @@ static void prep_new_page(struct page *page, unsigned int order, gfp_t gfp_flags
 {
 	post_alloc_hook(page, order, gfp_flags);
 
+	if (IS_ENABLED(CONFIG_PAGE_SANITIZE_VERIFY) && want_init_on_free()) {
+		int i;
+		for (i = 0; i < (1 << order); i++)
+			verify_zero_highpage(page + i);
+	}
+
 	if (!free_pages_prezeroed() && want_init_on_alloc(gfp_flags))
 		kernel_init_free_pages(page, 1 << order);
 
diff --git a/security/Kconfig.hardening b/security/Kconfig.hardening
index 80b3d4ba113f..6c84849d42d3 100644
--- a/security/Kconfig.hardening
+++ b/security/Kconfig.hardening
@@ -82,6 +82,13 @@ config INIT_ON_FREE_DEFAULT_ON
 	  touching "cold" memory areas. Most cases see 3-5% impact. Some
 	  synthetic workloads have measured as high as 8%.
 
+config PAGE_SANITIZE_VERIFY
+	bool "Verify sanitized pages"
+	default y
+	help
+	  When init_on_free is enabled, verify that newly allocated pages
+	  are zeroed to detect write-after-free bugs.
+
 endmenu
 
 endmenu
