From 3c0a4c175c8d9b826245fa6480ce5330bef68a6f Mon Sep 17 00:00:00 2001
From: Daniel Micay <danielmicay@gmail.com>
Date: Thu, 4 May 2017 15:58:57 -0400
Subject: [PATCH] slub: add support for verifying slab sanitization

This could support !PAGE_SANITIZE by zeroing in the object setup code in
those cases, but for now it takes the simple approach of depending on
page sanitization.

This is an extension to the sanitization feature in PaX for when
sacricifing more performance for security is acceptable.

Signed-off-by: Daniel Micay <danielmicay@gmail.com>
---
 init/Kconfig |  8 ++++++++
 mm/slub.c    | 31 +++++++++++++++++++++++++++----
 2 files changed, 35 insertions(+), 4 deletions(-)

diff --git a/init/Kconfig b/init/Kconfig
index 03d3ca8ac1ab..d21a2c3f53b0 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1771,6 +1771,14 @@ config SLAB_SANITIZE
 
 	  For slabs with debug poisoning enabling, this has no impact.
 
+config SLAB_SANITIZE_VERIFY
+	depends on SLAB_SANITIZE && PAGE_SANITIZE
+	default y
+	bool "Verify sanitized SLAB allocations"
+	help
+	  Verify that newly allocated slab allocations are zeroed to detect
+	  write-after-free bugs.
+
 config SLUB_CPU_PARTIAL
 	default y
 	depends on SLUB && SMP
diff --git a/mm/slub.c b/mm/slub.c
index 819e95e2efba..f712b8cb5db8 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -130,6 +130,11 @@ static inline bool has_sanitize(struct kmem_cache *s)
 	return IS_ENABLED(CONFIG_SLAB_SANITIZE) && !(s->flags & (SLAB_TYPESAFE_BY_RCU | SLAB_POISON));
 }
 
+static inline bool has_sanitize_verify(struct kmem_cache *s)
+{
+	return IS_ENABLED(CONFIG_SLAB_SANITIZE_VERIFY) && has_sanitize(s);
+}
+
 void *fixup_red_left(struct kmem_cache *s, void *p)
 {
 	if (kmem_cache_debug(s) && s->flags & SLAB_RED_ZONE)
@@ -1434,7 +1439,7 @@ static void setup_object(struct kmem_cache *s, struct page *page,
 {
 	setup_object_debug(s, page, object);
 	kasan_init_slab_obj(s, object);
-	if (unlikely(s->ctor)) {
+	if (unlikely(s->ctor) && !has_sanitize_verify(s)) {
 		kasan_unpoison_object_data(s, object);
 		s->ctor(object);
 		kasan_poison_object_data(s, object);
@@ -2735,7 +2740,14 @@ static __always_inline void *slab_alloc_node(struct kmem_cache *s,
 		stat(s, ALLOC_FASTPATH);
 	}
 
-	if (unlikely(gfpflags & __GFP_ZERO) && object)
+	if (has_sanitize_verify(s) && object) {
+		size_t offset = s->offset ? 0 : sizeof(void *);
+		BUG_ON(memchr_inv(object + offset, 0, s->object_size - offset));
+		if (s->ctor)
+			s->ctor(object);
+		if (unlikely(gfpflags & __GFP_ZERO) && offset)
+			memset(object, 0, sizeof(void *));
+	} else if (unlikely(gfpflags & __GFP_ZERO) && object)
 		memset(object, 0, s->object_size);
 
 	slab_post_alloc_hook(s, gfpflags, 1, &object);
@@ -2951,7 +2963,7 @@ static __always_inline void do_slab_free(struct kmem_cache *s,
 
 		while (1) {
 			memset(x + offset, 0, s->object_size - offset);
-			if (s->ctor)
+			if (!IS_ENABLED(CONFIG_SLAB_SANITIZE_VERIFY) && s->ctor)
 				s->ctor(x);
 			if (x == tail_obj)
 				break;
@@ -3174,7 +3186,18 @@ int kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
 	local_irq_enable();
 
 	/* Clear memory outside IRQ disabled fastpath loop */
-	if (unlikely(flags & __GFP_ZERO)) {
+	if (has_sanitize_verify(s)) {
+		int j;
+
+		for (j = 0; j < i; j++) {
+			size_t offset = s->offset ? 0 : sizeof(void *);
+			BUG_ON(memchr_inv(p[j] + offset, 0, s->object_size - offset));
+			if (s->ctor)
+				s->ctor(p[j]);
+			if (unlikely(flags & __GFP_ZERO) && offset)
+				memset(p[j], 0, sizeof(void *));
+		}
+	} else if (unlikely(flags & __GFP_ZERO)) {
 		int j;
 
 		for (j = 0; j < i; j++)
